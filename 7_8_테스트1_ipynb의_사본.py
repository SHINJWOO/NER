# -*- coding: utf-8 -*-
"""7/8 í…ŒìŠ¤íŠ¸1.ipynbì˜ ì‚¬ë³¸

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eh1b3OEMVhBSAFLWHGBet6LeD8hndHqH
"""

from google.colab import drive
drive.mount('/content/drive')

!apt-get update
!apt-get install g++ openjdk-8-jdk python3-dev
!pip install konlpy
!pip install transformers datasets scikit-learn matplotlib --quiet

# ğŸ”§ 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
import pandas as pd
import re
from konlpy.tag import Okt
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertForSequenceClassification
from torch.optim import AdamW
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, classification_report
import matplotlib.pyplot as plt
from tqdm import tqdm

# ğŸ§¼ 2. ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜
def load_stopwords(filepath):
    with open(filepath, 'r', encoding='utf-8') as f:
        stopwords = set(line.strip() for line in f if line.strip())
    return stopwords

def clean_text(text):
    text = re.sub(r'[^ê°€-í£a-zA-Z\s\-@\.]', '', text)
    return re.sub(r'\s+', ' ', text).strip()

tokenizer_korean = Okt()
stopwords = load_stopwords('/content/drive/MyDrive/DLP data/stopwords-ko.txt')

def tokenize_and_filter(text):
    cleaned = clean_text(text)
    tokens = tokenizer_korean.morphs(cleaned)
    return [t for t in tokens if t not in stopwords]

def preprocess_text(text):
    return ' '.join(tokenize_and_filter(text))

# ğŸ“ 3. ì—¬ëŸ¬ ê°œ ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì™€ í†µí•©
csv_paths = [
    '/content/drive/MyDrive/DLP data/___10MB________________.csv'
]

dfs = []
for path in csv_paths:
    df = pd.read_csv(path)
    df = df[['text', 'label']]
    df['label'] = df['label'].astype(int)
    df['text'] = df['text'].apply(preprocess_text)
    dfs.append(df)

# ğŸ”— í†µí•© ë° ì •ë¦¬
full_df = pd.concat(dfs, ignore_index=True)
full_df.dropna(inplace=True)
full_df = full_df.sample(frac=1).reset_index(drop=True)  # ì…”í”Œ

# ğŸ”€ 4. í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• 
train_texts, test_texts, train_labels, test_labels = train_test_split(
    full_df['text'].tolist(), full_df['label'].tolist(),
    test_size=0.2, stratify=full_df['label'], random_state=42
)

# ğŸ§¾ 5. HuggingFaceìš© Dataset ì •ì˜
tokenizer = BertTokenizer.from_pretrained('klue/bert-base')

class DLPDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoded = self.tokenizer(
            self.texts[idx],
            truncation=True,
            padding='max_length',
            max_length=self.max_len,
            return_tensors='pt'
        )
        return {
            'input_ids': encoded['input_ids'].squeeze(),
            'attention_mask': encoded['attention_mask'].squeeze(),
            'label': torch.tensor(self.labels[idx], dtype=torch.long)
        }

train_dataset = DLPDataset(train_texts, train_labels, tokenizer)
test_dataset = DLPDataset(test_texts, test_labels, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=10)

# âš™ï¸ 6. ëª¨ë¸ ë° í•™ìŠµ ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = BertForSequenceClassification.from_pretrained('klue/bert-base', num_labels=full_df['label'].nunique())
model.to(device)

optimizer = AdamW(model.parameters(), lr=2e-5)
loss_fn = torch.nn.CrossEntropyLoss()
train_losses = []

# ğŸš€ 7. í•™ìŠµ ë£¨í”„
epochs = 2
for epoch in range(epochs):
    model.train()
    total_loss = 0
    loop = tqdm(train_loader, desc=f'Epoch {epoch+1}')
    for batch in loop:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = loss_fn(outputs.logits, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        loop.set_postfix(loss=loss.item())
    train_losses.append(total_loss / len(train_loader))

# ğŸ“Š 8. í‰ê°€ ë° ì‹œê°í™”
model.eval()
preds, truths = [], []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        pred = torch.argmax(outputs.logits, dim=1)
        preds.extend(pred.cpu().numpy())
        truths.extend(labels.cpu().numpy())

print("ğŸ“Œ Accuracy:", accuracy_score(truths, preds))
print("ğŸ“Œ F1 Score:", f1_score(truths, preds, average='macro'))
print("\nğŸ“Œ Classification Report:\n", classification_report(truths, preds))

plt.plot(train_losses)
plt.title("Training Loss (Multi-Set BERT)")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.grid(True)
plt.show()

new_df = pd.read_csv('/content/drive/MyDrive/DLP data/faker_dlp_dataset.csv')
new_df = new_df[['text', 'label']].dropna()
new_df['label'] = new_df['label'].astype(int)
new_df['text'] = new_df['text'].apply(preprocess_text)  # ê¸°ì¡´ ì „ì²˜ë¦¬ í•¨ìˆ˜ í™œìš©

new_dataset = DLPDataset(new_df['text'], new_df['label'], tokenizer)
new_loader = DataLoader(new_dataset, batch_size=16)

model.eval()
from sklearn.metrics import accuracy_score, f1_score, classification_report

all_preds, all_labels = [], []

with torch.no_grad():
    for batch in new_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs.logits, dim=1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

print("ğŸ“Š Accuracy:", accuracy_score(all_labels, all_preds))
print("ğŸ“Š F1 Score:", f1_score(all_labels, all_preds, average='macro'))
print("\nğŸ“‹ Classification Report:\n", classification_report(all_labels, all_preds))

new_df = pd.read_csv('/content/drive/MyDrive/DLP data/sensitive_data_dataset.csv')
new_df = new_df[['text', 'label']].dropna()
new_df['label'] = new_df['label'].astype(int)
new_df['text'] = new_df['text'].apply(preprocess_text)  # ê¸°ì¡´ ì „ì²˜ë¦¬ í•¨ìˆ˜ í™œìš©

new_dataset = DLPDataset(new_df['text'], new_df['label'], tokenizer)
new_loader = DataLoader(new_dataset, batch_size=16)

model.eval()
from sklearn.metrics import accuracy_score, f1_score, classification_report

all_preds, all_labels = [], []

with torch.no_grad():
    for batch in new_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs.logits, dim=1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

print("ğŸ“Š Accuracy:", accuracy_score(all_labels, all_preds))
print("ğŸ“Š F1 Score:", f1_score(all_labels, all_preds, average='macro'))
print("\nğŸ“‹ Classification Report:\n", classification_report(all_labels, all_preds))

